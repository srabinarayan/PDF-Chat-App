{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader,PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/mnt/d/LANGCHAIN_UDEMY/usecase/PDF-Chat App/MobileNets Efficient Convolutional Neural Networks for Mobile Vision.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Using cached pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Using cached pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-4.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"Data\"\n",
    "if os.path.exists(pdf_path) and len(os.listdir(pdf_path))>0: \n",
    "    loader = PyPDFDirectoryLoader(pdf_path)\n",
    "else:\n",
    "    print(\"Pass\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], optional_variables=['chat_history'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'langchain-ai', 'lc_hub_repo': 'retrieval-qa-chat', 'lc_hub_commit_hash': 'b60afb6297176b022244feb83066e10ecadcda7b90423654c4a9d45e7a73cebc'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='Answer any use questions based solely on the context below:\\n\\n<context>\\n{context}\\n</context>')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "hub.pull(\"langchain-ai/retrieval-qa-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(pdf_path)\n",
    "raw_documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 600,chunk_overlap = 50)\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of OpenAI LLM\n",
    "llm = ChatOpenAI(temperature=0.1, verbose=True)\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfChatApp():\n",
    "    def __init__(self,file_path: str, embeddings, llm, vectorstore_dir) -> None:\n",
    "        self.file_path = file_path\n",
    "        self.embeddings = embeddings\n",
    "        self.vectorstores_dir = vectorstore_dir\n",
    "        self.search_type = \"similarity\"\n",
    "        self.top_n = 3\n",
    "        self.llm = llm\n",
    "        pass\n",
    "    def data_ingestion(self,file_path):\n",
    "        # Create and load PDF Loader\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        raw_documents = loader.load()\n",
    "        print(f\"Loaded {len(raw_documents)} documents\")\n",
    "        # Split Text from pdf \n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size = 600,chunk_overlap = 50)\n",
    "        documents = text_splitter.split_documents(raw_documents)\n",
    "        return documents\n",
    "    def get_vector_store(self, docs, embeddings, vectorstore_dir):\n",
    "        vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "        vectorstore.save_local(vectorstore_dir)\n",
    "        return None\n",
    "    \n",
    "    def load_retriever(self, embeddings,vectorstore_dir):\n",
    "        retriever = (\n",
    "            FAISS\n",
    "            .load_local(vectorstore_dir,embeddings,allow_dangerous_deserialization=True)\n",
    "            .as_retriever(search_type = self.search_type, search_kwargs = {\"k\":self.top_n})\n",
    "        )\n",
    "        return retriever\n",
    "    def get_conversational_chain(self):\n",
    "        docs = self.data_ingestion(file_path=self.file_path)\n",
    "        _ = self.get_vector_store(\n",
    "            docs= docs,\n",
    "            embeddings = self.embeddings,\n",
    "            vectorstore_dir = self.vectorstores_dir\n",
    "        )\n",
    "        retriever = self.load_retriever(self.embeddings,self.vectorstores_dir)\n",
    "        # 2. Incorporate the retriever into a question-answering chain.\n",
    "        system_prompt = (\n",
    "            \"You are an assistant for question-answering tasks. \"\n",
    "            \"Answer the question as detailed as possible from the provided context, make sure to provide all the details\" \n",
    "            \"if the answer is not in provided context just say, answer is not available in the context, don't provide the wrong answer\"\n",
    "            \"\\n\\n\"\n",
    "            \"{context}\"\n",
    "        )\n",
    "\n",
    "        retrieval_qa_template = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_prompt),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # retrieval_qa_template = PromptTemplate(template = template, input_variables = [\"context\", \"question\"])\n",
    "        document_chain = create_stuff_documents_chain(self.llm, retrieval_qa_template)\n",
    "        retrieval_chain = create_retrieval_chain(\n",
    "            retriever=retriever, combine_docs_chain=document_chain\n",
    "        )\n",
    "        \n",
    "        return retrieval_chain\n",
    "    def get_llm_response(self,chain, question):\n",
    "        return chain.invoke({\"input\": question})['answer']\n",
    "    \n",
    "    def get_llm_response(self,chain, question):\n",
    "        return chain.invoke({\"input\": question})['answer']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import os ,shutil\n",
    "def clear_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        # Remove all the files in the directory\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "class PdfChatApp():\n",
    "    def __init__(self, embeddings, llm, vectorstore_dir) -> None:\n",
    "        self.file_path = None\n",
    "        self.embeddings = embeddings\n",
    "        self.vectorstores_dir = vectorstore_dir\n",
    "        self.search_type = \"similarity\"\n",
    "        self.top_n = 3\n",
    "        self.llm = llm\n",
    "        pass\n",
    "    def store_files(self,files):\n",
    "        current_directory = os.path.dirname(os.path.abspath(__file__))\n",
    "        UPLOAD_FOLDER = os.path.join(current_directory, \"RAW_DATA\")\n",
    "        clear_folder(UPLOAD_FOLDER)\n",
    "        if not os.path.exists(UPLOAD_FOLDER):\n",
    "            os.makedirs(UPLOAD_FOLDER,exist_ok=True)  \n",
    "        for file in files:\n",
    "            file = file.name\n",
    "            file_name = file.split(os.sep)[-1]\n",
    "            shutil.copy(file,os.path.join(UPLOAD_FOLDER,file_name))\n",
    "        self.file_path =  UPLOAD_FOLDER\n",
    "        return None   \n",
    "    def data_ingestion(self,file_path):\n",
    "        # Create and load PDF Loader\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        raw_documents = loader.load()\n",
    "        print(f\"Loaded {len(raw_documents)} documents\")\n",
    "        # Split Text from pdf \n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size = 600,chunk_overlap = 50)\n",
    "        documents = text_splitter.split_documents(raw_documents)\n",
    "        return documents\n",
    "    def get_vector_store(self, docs, embeddings, vectorstore_dir):\n",
    "        vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "        vectorstore.save_local(vectorstore_dir)\n",
    "        return None\n",
    "    \n",
    "    def load_retriever(self, embeddings,vectorstore_dir):\n",
    "        retriever = (\n",
    "            FAISS\n",
    "            .load_local(vectorstore_dir,embeddings,allow_dangerous_deserialization=True)\n",
    "            .as_retriever(search_type = self.search_type, search_kwargs = {\"k\":self.top_n})\n",
    "        )\n",
    "        return retriever\n",
    "    def get_conversational_chain(self):\n",
    "        docs = self.data_ingestion(file_path=self.file_path)\n",
    "        _ = self.get_vector_store(\n",
    "            docs= docs,\n",
    "            embeddings = self.embeddings,\n",
    "            vectorstore_dir = self.vectorstores_dir\n",
    "        )\n",
    "        retriever = self.load_retriever(self.embeddings,self.vectorstores_dir)\n",
    "        # 2. Incorporate the retriever into a question-answering chain.\n",
    "        system_prompt = (\n",
    "            \"You are an assistant for question-answering tasks. \"\n",
    "            \"Answer the question as detailed as possible from the provided context, make sure to provide all the details\" \n",
    "            \"if the answer is not in provided context just say, answer is not available in the context, don't provide the wrong answer\"\n",
    "            \"\\n\\n\"\n",
    "            \"{context}\"\n",
    "        )\n",
    "\n",
    "        retrieval_qa_template = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_prompt),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # retrieval_qa_template = PromptTemplate(template = template, input_variables = [\"context\", \"question\"])\n",
    "        document_chain = create_stuff_documents_chain(self.llm, retrieval_qa_template)\n",
    "        retrieval_chain = create_retrieval_chain(\n",
    "            retriever=retriever, combine_docs_chain=document_chain\n",
    "        )\n",
    "        \n",
    "        return retrieval_chain\n",
    "    def get_llm_response(self,chain, question):\n",
    "        return chain.invoke({\"input\": question})['answer']\n",
    "    \n",
    "    def get_llm_response(self,chain, question):\n",
    "        return chain.invoke({\"input\": question})['answer']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil\n",
    "def clear_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        # Remove all the files in the directory\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "vectorstore_dir = \"vectordb\"\n",
    "if not os.path.exists(vectorstore_dir):\n",
    "    os.makedirs(vectorstore_dir,exist_ok=True)\n",
    "clear_folder(vectorstore_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 documents\n"
     ]
    }
   ],
   "source": [
    "pdfchatapp = PdfChatApp(pdf_path,embeddings,llm,\"PDF-Chat App/vectordb\")\n",
    "generator_chain = pdfchatapp.get_conversational_chain()\n",
    "query = \"What is MobileNet architrecture ?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The MobileNet architecture is based on depthwise separable convolutions. It consists of core layers built on depthwise separable filters, where the standard convolutional filters are replaced by two layers: depthwise convolution and pointwise convolution. The network structure of MobileNet primarily utilizes depthwise separable convolutions, except for the first layer which is a full convolution. Each layer in MobileNet is followed by batch normalization and ReLU nonlinearity, except for the final fully connected layer which feeds into a softmax layer for classification. The architecture allows for easy exploration of network topologies to find an efficient network design. Additionally, MobileNet architecture includes model shrinking hyperparameters like width multiplier and resolution multiplier for optimization.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfchatapp.get_llm_response(generator_chain,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TemporaryDirectory '/tmp/tmpeb6bkv9m'>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempfile.TemporaryDirectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ,shutil\n",
    "def clear_folder(folder_path):\n",
    "    if os.path.exists(folder_path):\n",
    "        # Remove all the files in the directory\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}. Reason: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 23:06:19.209 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/srabinarayan/miniconda3/envs/genai_dev/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-09-10 23:06:19.215 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "# Define the file uploader and submit button in a tab\n",
    "def file_uploader_tab():\n",
    "    st.header(\"File Uploader Tab\")\n",
    "    uploaded_file = st.file_uploader(\"Upload a file\", type=[\"txt\", \"csv\", \"xlsx\"])\n",
    "    if uploaded_file is not None:\n",
    "        st.write(\"File uploaded:\", uploaded_file.name)\n",
    "    if st.button(\"Submit\"):\n",
    "        st.write(\"Submit button clicked\")\n",
    "\n",
    "# Define the main page layout\n",
    "def main_page():\n",
    "    st.title(\"Main Page\")\n",
    "    \n",
    "    # Row 1: Chat Interface\n",
    "    st.subheader(\"Chat Interface\")\n",
    "    chat_placeholder = st.empty()\n",
    "    user_message = st.text_input(\"Enter your message\")\n",
    "    if st.button(\"Send\"):\n",
    "        chat_placeholder.write(f\"User: {user_message}\")\n",
    "\n",
    "    # Row 2: Placeholder for Text Message and Submit Button\n",
    "    st.subheader(\"Text Message Input\")\n",
    "    text_message = st.text_area(\"Type your message here\")\n",
    "    if st.button(\"Submit Message\"):\n",
    "        st.write(f\"Message Submitted: {text_message}\")\n",
    "\n",
    "    # Row 3: Click Button\n",
    "    st.subheader(\"Click Button\")\n",
    "    if st.button(\"Click Me\"):\n",
    "        st.write(\"Button Clicked\")\n",
    "\n",
    "# Create the Streamlit app layout\n",
    "def main():\n",
    "    st.sidebar.title(\"Navigation\")\n",
    "    option = st.sidebar.selectbox(\"Choose a tab\", [\"Main Page\", \"File Uploader Tab\"])\n",
    "    \n",
    "    if option == \"Main Page\":\n",
    "        main_page()\n",
    "    elif option == \"File Uploader Tab\":\n",
    "        file_uploader_tab()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
